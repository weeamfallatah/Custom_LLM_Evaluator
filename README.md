# Custom LLM Evaluator (C-Eval)

**C-Eval** is a framework for evaluating Large Language Model (LLM) outputs using **custom, user-defined metrics** with integrated **Chain-of-Thought (CoT)** reasoning to enhance evaluation steps.

## Features
- **Define your own metrics** in plain language or load them from JSON files.
- Automatically **generate or refine evaluation steps** based on given criteria.
- Evaluate outputs considering **context**, **expected outputs**, and **queries**.
- Get **scored results** (0â€“1) with detailed reasoning for each metric.

## Key Capabilities
- Step improvement with correct/incorrect examples.
- Support for datasets from CSV or Excel.
- Structured results including scores, reasoning, and step improvements.
- Flexible for both simple and complex evaluation tasks.

## Use Cases
- Assessing response quality in NLP applications.
- Testing accuracy and safety in AI-generated outputs.
- Building domain-specific evaluation frameworks.

## Example Workflow
1. Load your dataset (CSV or Excel).
2. Define or import custom metrics.
3. Run C-Eval to score each test case.
4. Review results with scores, reasoning, and improvements.

> ðŸš€ **Future Enhancements:** Step refinement with user feedback and score normalization for higher accuracy.

## Highâ€‘level architecture
custom_metric_evaluator.py
â”œâ”€ run_model_octo_ai()         # Thin OctoAI chat completion wrapper
â”œâ”€ TestCaseParams              # String constants for dataframe keys
â”œâ”€ Eval_metric                 # Container for a single metric's config
â””â”€ CustomLLMEvaluator          # Orchestrates step-gen, step-improve, scoring
   â”œâ”€ run_model()                        # Model call helper
   â”œâ”€ generate_eval_steps()              # CoT â†’ initial steps from criteria
   â”œâ”€ generate_improving_steps()         # Refine steps (optional examples)
   â”œâ”€ extract_improving_steps_response() # Parse JSON/regex â†’ DataFrame
   â”œâ”€ improved_steps_check()             # Cache per-metric refined steps
   â”œâ”€ custom_evaluator_prompt()          # Score one test case
   â””â”€ custom_Eval()                      # Score a DataFrame of cases

