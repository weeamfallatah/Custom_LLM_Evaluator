{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes In this version 12.2:\n",
    "- The final evaluation prompt has been modified and improved.\n",
    "- The code has been split into two separate files for ease of use by the user.\n",
    "- The final outputs that will be displayed to the user have been adjusted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from custom_metric_evaluator import TestCaseParams, Eval_metric, CustomLLMEvaluator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test cases and metrics info form JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file_path):\n",
    "    # ----------------------------------------------------- Load and Process Data -----------------------------------------------------------------------------------\n",
    "    # Load the JSON file with utf-8 encoding\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Extract the \"Actual_Responses\" list and context\n",
    "    actual_responses = data.get(\"Actual_Responses\", [])\n",
    "    context = data.get(\"Context\", [])\n",
    "\n",
    "    # Create a DataFrame for test cases\n",
    "    TestCases_df = pd.DataFrame({'query': \"\", 'context': context, 'expected_output': \"\", 'actual_output': actual_responses})\n",
    "    \n",
    "    # Normalize the 'metrics' data\n",
    "    metrics_df = pd.json_normalize(data['metrics'])\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return TestCases_df , metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LLM Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_cases(TestCases_df , metrics_df):\n",
    "    # ----------------------------------------------------- Initialize Evaluator -----------------------------------------------------------------------------------\n",
    "    # Create evaluator instance\n",
    "    custom_llm_evaluator = CustomLLMEvaluator()\n",
    "\n",
    "    # ----------------------------------------------------- Evaluate test cases with multi metrics -----------------------------------------------------------------------------------\n",
    "    custom_Eval_all_results = []\n",
    "\n",
    "    # Loop through each row in the metrics DataFrame\n",
    "    for index, row in metrics_df.iterrows():\n",
    "        # Define the custom evaluation parameters based on the metric\n",
    "        evaluation_params = [TestCaseParams.ACTUAL_OUTPUT]  # Always evaluate the actual output\n",
    "\n",
    "        # Add 'context' as a parameter if 'Uses_Context' is True\n",
    "        if row[\"Uses_Context\"]:\n",
    "            evaluation_params.append(TestCaseParams.CONTEXT)\n",
    "\n",
    "        # Create an Eval_metric object for each metric\n",
    "        metric = Eval_metric(\n",
    "            metric_name=row['Metric_Name'],\n",
    "            criteria=row['Criteria'],\n",
    "            eval_steps=row['Evaluation_Steps'],\n",
    "            eval_steps_correct_example=row['Correct_Eval_steps_example'],\n",
    "            eval_steps_incorrect_example=row['Incorrect_Eval_steps_example'],\n",
    "            uses_context=row['Uses_Context'],\n",
    "            evaluation_params=evaluation_params\n",
    "        )\n",
    "\n",
    "        # Evaluate test cases for the current metric\n",
    "        custom_Eval_result = custom_llm_evaluator.custom_Eval(TestCases=TestCases_df, metric=metric)\n",
    "\n",
    "        # Append the results\n",
    "        custom_Eval_all_results.append(custom_Eval_result)\n",
    "\n",
    "    # Concatenate all results into one DataFrame\n",
    "    custom_Eval_all_results = pd.concat(custom_Eval_all_results, ignore_index=True)\n",
    "\n",
    "    # ----------------------------------------------------- Process the evaluation results -----------------------------------------------------------------------------------\n",
    "    # Define the columns that will always be present in the output\n",
    "    base_columns = [\"query\", \"context\", \"expected_output\", \"actual_output\"]\n",
    "    \n",
    "    # Define the columns that contain evaluation data specific to each metric\n",
    "    pivot_columns = ['scores', 'final_score', 'reasoning', 'improved_steps_response', 'evaluation_result']\n",
    "    \n",
    "    # Create a base DataFrame with distinct test cases\n",
    "    base_df = custom_Eval_all_results[base_columns].drop_duplicates()\n",
    "\n",
    "    # Pivot the custom evaluation results to organize by 'metric_name'\n",
    "    metrics_result_df = custom_Eval_all_results.pivot_table(index=base_columns, columns='metric_name', values=pivot_columns, aggfunc='first')\n",
    "\n",
    "    # Reorder the columns in the metrics result DataFrame based on 'new_order'\n",
    "    new_order = ['scores', 'final_score', 'reasoning', 'evaluation_result', 'improved_steps_response']\n",
    "    metrics_result_df = metrics_result_df[new_order]\n",
    "\n",
    "    # Flatten the MultiIndex columns by combining metric name with column names\n",
    "    metrics_result_df.columns = [f\"{col}_{metric}\" for metric, col in metrics_result_df.columns]\n",
    "    metrics_result_df.reset_index(inplace=True)\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return metrics_result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'RFP_correctClarificationGeneration.json'\n",
    "#file_path = 'GEval_Context_InvalidDataGen_me_test.json'\n",
    "TestCases_df , metrics_df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestCases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_test_cases(TestCases_df , metrics_df)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(f\"custom_Eval_result_v2.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
